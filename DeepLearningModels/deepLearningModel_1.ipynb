## Objective :

To predict which customer is more likely to purchase the newly introduced telecom plan.
## The Plan

We will be developing 2 models: Logistic Regression with Regularization and Deep Neural Network techniques for this problem and compare them and their performances. We will be picking the model that suits the problem better and gives a more accurate outcome. We will try to uncover the reasons why a model gives more accurate results.

## Data Structure

### Customer Details

| Entity             | Explanation                                               |
|--------------------|-----------------------------------------------------------|
| CustomerID         | Unique customer ID                                        |
| PlanTaken          | Plan Purchase (0: No, 1: Yes)                             |
| Age                | Age of customer                                           |
| TypeofContact      | Contact Type (Company Invited or Self Inquiry)             |
| CityTier           | City Tier (Tier 1 > Tier 2 > Tier 3)                       |
| Occupation         | Occupation of customer                                    |
| Gender             | Gender of customer                                        |
| NumberOfPersons    | Number of persons for the plan (Friends and Family)        |
| PreferredServiceStar| Preferred service rating by customer                       |
| MaritalStatus      | Marital status of customer                                |
| NumberOfUpgrades   | Average number of upgrades in a year                       |
| iPhone             | Has iPhone (0: No, 1: Yes)                                |
| PhoneContract      | Has contracted phone (0: No, 1: Yes)                       |
| NumberOfChildren   | Number of children for the plan                            |
| Designation        | Customer's designation in the current organization        |
| MonthlyIncome      | Gross monthly income of the customer                       |


### Customer Interaction Data

| Entity                  | Explanation                                           |
|-------------------------|-------------------------------------------------------|
| PitchSatisfactionScore  | Satisfaction with the sales pitch (1-5 scale)         |
| PlanPitched             | Plan proposed by the salesperson                       |
| NumberOfFollowups      | Total follow-ups conducted after the sales pitch       |
| DurationOfPitch         | Duration of the sales pitch to the customer             |

## Models
# Reference :- Code exercises provided by Prof. Bhavik Gandhi in the previous lectures on Loyalist College moodle site.
## 1. Logistic Regression
### Importing all important libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score
import scipy.stats as stats
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import roc_auc_score, roc_curve
import warnings
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, recall_score
from sklearn.linear_model import LogisticRegression
warnings.filterwarnings('ignore')
### Data Preprocessing and loading for LR
#### Reading the data
tel=pd.read_csv(r"/content/Telecome.csv")
# coping the data into another dataframe to work
dataframe = tel
# viewing the data head and tail
dataframe.head()
dataframe.tail()
# Checking the data shape
dataframe.shape
#### Checking the datatypes of all the attributes
dataframe.info()
#### The dataset contains null values in the Age, DurationOfPitch, NumberOfFollowups, PreferredServiceStar, NumberOfUpgrades, NumberOfChildren, MonthlyIncome
# To handle the null values we are dropping the null value column

dataframe.dropna(subset=['Age', 'DurationOfPitch', 'NumberOfFollowups', 'PreferredServiceStar', 'NumberOfUpgrades', 'NumberOfChildren', 'MonthlyIncome'], inplace=True)
dataframe.info()
dataframe.isnull().sum()
dataframe.describe()
#### Removing the unwanted column from dataframe
dataframe.drop('Designation',axis='columns', inplace=True)
dataframe.drop('CustomerID',axis='columns', inplace=True)
dataframe.drop('iPhone',axis='columns', inplace=True)
dataframe.drop('PhoneContract',axis='columns', inplace=True)
#### Fixing the data types
TypeofContact,Occupation, Gender, PlanPitched, MaritalStatus and Designation are of object type, we can change them to categories.
dataframe['TypeofContact'] = pd.Categorical(dataframe['TypeofContact'])
dataframe['Gender'] = pd.Categorical(dataframe['Gender'])
dataframe['Occupation'] = pd.Categorical(dataframe['Occupation'])
dataframe['PlanPitched'] = pd.Categorical(dataframe['PlanPitched'])
dataframe['MaritalStatus'] = pd.Categorical(dataframe['MaritalStatus'])
dataframe.describe(include=['category'])
# Coverting the categorial
dataframe = pd.get_dummies(dataframe, sparse=True,drop_first=True)
dataframe.head()
# get the target data
y = dataframe['PlanTaken']
X = dataframe.drop(['PlanTaken'], axis = 1)
X.head()
X.shape
X.isnull().sum()
### Model Building for LR
# Splitting dataframe into training and test set
X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
#### l2 refer regulaziation
# crating the logistic regession model
logreg = LogisticRegression(penalty='l2', random_state=21)
# train the model
logreg.fit(X_train, y_train)
# make prediction on the test data
y_perdiction = logreg.predict(X_test)
print(f'Accuracy: {logmodel.score(X, y):.3f}')

#calculate AUC_score
y_prob = logreg.predict_log_proba(X_test)[:,1]
auc_score = roc_auc_score(y_test, y_prob)
print(f"AUC_Score:{auc_score}")
# Perform cross-validation with stratified k-fold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(logreg, X_train, y_train, cv=cv, scoring='accuracy')
#plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label='ROC Curve')
plt.plot([0,1], [0,1], linestyle='--', color='gray', label='Random Guessing')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive rate (TPR)')
plt.title('Receiver operating Characteristic (ROC) Curve')
plt.legend()
plt.show()
### Model Tunning for LR
from sklearn.model_selection import GridSearchCV
# create logistic regression_model
logreg = LogisticRegression(random_state=21)
# define hyperparameter and possible values
param_grid = { 'penalty':['l1', 'l2', 'elasticnet', 'none'],
              'C': np.logspace(-4, 4, 20),
               'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],
               }
clf = GridSearchCV(logreg, param_grid= param_grid, cv= 5, verbose=True, n_jobs=1, )
best_clf = clf.fit(X,y)
best_clf.best_estimator_
print(f'Accuracy: {logmodel.score(X, y):.3f}')
# prediction for test set using above find result the best model
y_perdiction = best_clf.predict(X_test)
#### No major change in accuracy
# evaluate the model
conf_matrix = confusion_matrix(y_test, y_perdiction)
class_report = classification_report(y_test, y_perdiction)
print("Confusion matrix:")
print("conf_matrix")
print("Classification Report:")
print(class_report)
#### AUC score is boost by 4
# calculate Auc score for this model
y_prob = best_clf.predict_proba(X_test)[:,1]
auc_score = roc_auc_score(y_test, y_prob)
print(f"AUC Score: {auc_score}")
# now ploting Roc cure for this model
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label='ROC Curve')
plt.plot([0,1], [0,1], linestyle='--', color='gray', label='Random Guessing')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) curve')
plt.legend()
plt.show()
## 2. DNN
### Importing the libraries for DNN
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score
import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Input
from keras.layers import Dense, Dropout, Flatten, BatchNormalization
from keras.callbacks import LearningRateScheduler
from keras.callbacks import EarlyStopping
from keras.layers import Dense, Activation, Dropout
from keras.utils import to_categorical, plot_model
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
import warnings
warnings.filterwarnings('ignore')
### Data Preprocessing and loading for DNN
tel=pd.read_csv(r"Telecom.csv")
# coping the data into another dataframe to work
dataframe = tel
# viewing the data head and tail
dataframe.head()
dataframe.tail()
# Checking the data shape
dataframe.shape
It has 4888 rows and 20 columns
# Checking the datatypes of all the attributes
dataframe.info()
The dataset contains null values in the Age, DurationOfPitch, NumberOfFollowups, PreferredServiceStar, NumberOfUpgrades, NumberOfChildren, MonthlyIncome
#fill missing value of numerical features with mean
telecom_numeric_col=dataframe.select_dtypes(include=['int','float']).columns
dataframe[telecom_numeric_col]=dataframe[telecom_numeric_col].apply(lambda col:col.fillna(col.mean()))
dataframe.info()
dataframe.isnull().sum()
#To handle the null values we are dropping the null value column
dataframe.dropna(subset=['TypeofContact'], inplace=True)
Now there is no null values in the dataset
dataframe.describe()
### Fixing the data types
* TypeofContact,Occupation, Gender, PlanPitched, MaritalStatus and Designation  are of object type, we can change them to categories.
dataframe['TypeofContact'] = pd.Categorical(dataframe['TypeofContact'])
dataframe['Gender'] = pd.Categorical(dataframe['Gender'])
dataframe['Occupation'] = pd.Categorical(dataframe['Occupation'])
dataframe['PlanPitched'] = pd.Categorical(dataframe['PlanPitched'])
dataframe['MaritalStatus'] = pd.Categorical(dataframe['MaritalStatus'])
dataframe.describe(include=['category'])
cols_category= dataframe.select_dtypes(['category'])
for i in cols_category.columns:
    print("Unique values in", i, 'are :')
    print(cols_category[i].value_counts())
    print('*'*50)
#### Handling the Categorial Features
# Doing One hot encoding for categorial features
dataframe = pd.get_dummies(dataframe, sparse=True,drop_first=True)
#stacked bar chart comparing "city tier " with plantaken
city_tier_plan_taken = pd.crosstab(dataframe['CityTier'], dataframe['PlanTaken'])
city_tier_plan_taken.plot(kind='bar', stacked=True, figsize=(10,6))
plt.title('CityTier Vs PlanTaken')
plt.xlabel('CityTier')
plt.ylabel('Count')
plt.show()
dataframe.head()
#### Removing the unwanted column from dataframe
dataframe.drop('Designation',axis='columns', inplace=True)
dataframe.drop('CustomerID',axis='columns', inplace=True)
dataframe.drop('iPhone',axis='columns', inplace=True)
dataframe.drop('PhoneContract',axis='columns', inplace=True)
### Data Visulization and analysis for DNN
#univariative Analysis
import matplotlib.pyplot as plt
import seaborn as sns

def histogram_boxplot(data1, feature, figsize=(12, 7), kde=False, bins=None):
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,
        sharex=True,
        gridspec_kw={'height_ratios': (0.50, 1.0)},
        figsize=figsize,
    )
    sns.boxplot(
        data=data1, x=feature, ax=ax_box2, showmeans=True, color="pink"
    )
    sns.histplot(
        data=data1, x=feature, ax=ax_hist2, bins=bins, palette="blue"
    ) if bins else sns.histplot(
        data=data1, x=feature, kde=kde, ax=ax_hist2
    )
    ax_hist2.axvline(
        data1[feature].mean(), color="red", linestyle="--"
    )
    ax_hist2.axvline(
        data1[feature].median(), color="black", linestyle="--"
    )

#observations on age
histogram_boxplot(dataframe,'Age' )
#### This one is right skewed and on average the members age is 36. There are no outliers.
#observation on PlanTaken
histogram_boxplot(dataframe, 'PlanTaken')
#### Most of the members in this are taken the paln and there are two outliers
#observation on CityTier
histogram_boxplot(dataframe,'CityTier')
#### This is rightskewed and most of the members are fron tier1.
#observation on durationofpitch
histogram_boxplot(dataframe, 'DurationOfPitch')
#### This is right skewed and one otlier is there. The average duration pitch is 700.
#observation on NumberOfPersons
histogram_boxplot(dataframe, 'NumberOfPersons')
#### This one is left skewed and one oulier is there.
#observation on numberoffollowups
histogram_boxplot(dataframe, 'NumberOfFollowups')
#### Numberoffollowups are left skewed and 2000 members came for 4 followups. There are two ouliers
#observation on PreferredServiceStar
histogram_boxplot(dataframe, 'PreferredServiceStar')
#### Nearly 2900 members gave us 3 star rating and it is right skewed.
#observation on NumberOfUpgrages
histogram_boxplot(dataframe, 'NumberOfUpgrades')
#### On average number of upgrades customers got are 5 abd there are 5 outliers.
#observation on Iphone
histogram_boxplot(dataframe, 'iPhone')
#### In this around 3400 members does not iphone and 1300 membesr have the i phone. It is right skewed.
#observation on pitchsatisfactionscore
histogram_boxplot(dataframe, 'PitchSatisfactionScore')
#### This pitchsatisfationscore is also right skewed and the average lies between 3
#observation on number of children
histogram_boxplot(dataframe, "NumberOfChildren")
#### Observations on Numberofchildren is right skewed
#observation on PhoneContract
histogram_boxplot(dataframe, 'PhoneContract')
#### From the above observations on phone contract is
* 1800 members have zero phone contracts
* 2900 members habe atleat 1 phone contract
#observation on Monthly Income
histogram_boxplot(dataframe, 'MonthlyIncome')
#### From the observation of the above graph
* On average employee average income lies between 20000 to 21000 and monthly is right skewed
#Bivariative Analysis
numeric_data = dataframe.select_dtypes(include='number')
plt.figure(figsize=(20, 10))
sns.heatmap(numeric_data.corr(), annot=True)
plt.xticks(rotation=45)
plt.yticks(rotation=45)
#Barchart
categories = dataframe['MonthlyIncome']
values=dataframe['PitchSatisfactionScore']
plt.bar(categories, values, color='blue')
plt.xlabel('MonthlyIncome')
plt.ylabel('PitchSatisfactionScore')
plt.title("Comparision of MonthlyIncome and PitchSatisfactionScore")
plt.show()
### Model Building for DNN

To Handle significant imbalance in the distribution of the target classes, we are using stratified sampling to ensure that  class frequencies are approximately preserved in train and test sets.
y = dataframe['PlanTaken']
X = dataframe.drop(['PlanTaken'],axis=1)
# Splitting dataframe into training and test set
X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)
print(X_train.shape, X_test.shape)
# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
y.value_counts(1)
y_train.value_counts(1)
y_test.value_counts(1)
### Model Evaluation for DNN
# creating a function to compute the classification matrics for the classification model
def model_classification_matrics(classifier, test_x, test_y):
    """
    Fn to calculate the confusion matrix

    classifier: actual model classifier
    test_x: independent variables
    test_y: dependent variable
    """

    # predicting the test data
    pred = (classifier.predict(X_test) > 0.5).astype(int)

    accuracy = accuracy_score(test_y, pred)  # calculating Accuracy
    recall = recall_score(test_y, pred)  # Calculating Recall
    precision = precision_score(test_y, pred)  # Calculating Precision
    f1 = f1_score(test_y, pred)  # Calculating  F1-score


    # creating a dataframe of metrics
    metrix_data = pd.DataFrame(
        {
            "Accuracy": accuracy,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return metrix_data
# creating a function to compute the confusion matrix matrics for the classification model
def confusion_matrix_fun(classifier, test_x, test_y):
    """
    To plot the confusion_matrix with percentages

    classifier: actual model classifier
    test_x: independent variables
    test_y: dependent variable
    """
    pred = (classifier.predict(X_test) > 0.5).astype(int)

    confusion_m = confusion_matrix(test_y, pred)
    # Plot confusion matrix using seaborn
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_m, annot=True, fmt="d", cmap="Blues", cbar=False,
                xticklabels=np.unique(test_y), yticklabels=np.unique(test_y))
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Calculating the AUC Score
    auc_score = roc_auc_score(y_test, pred)
    print(f"AUC Score: {auc_score}")

    #Plotting the ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, pred)
    plt.figure(figsize=(8,6))
    plt.plot(fpr, tpr, color='blue', label='ROC Curve')
    plt.plot([0,1], [0,1], linestyle='--', color='gray', label='Random Guessing')
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True positive Rate (TPR)')
    plt.title('Receiver Operating Characteristic (ROC) curve')
    plt.legend()
    plt.show()
#Network paramters
batch_size = 130
dropout = 0.45
input_size = X_train.shape[1]
input_size
#Model with 5 layer with Relu function
model = Sequential()
model.add(Dense(2048, input_dim=input_size))
model.add(Activation('relu'))
model.add(Dense(512, input_dim=input_size))
model.add(Activation('relu'))
model.add(Dense(128,input_dim=input_size))
model.add(Activation('relu'))
model.add(Dense(32,input_dim=input_size))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
#Compiling the model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.summary()
plot_model(model,to_file='model_1.png',show_shapes=True)
#Training the model
model.fit(X_train,y_train,epochs=10,batch_size=batch_size)
#Calculating the accuracy on test data
loss, acc = model.evaluate(X_test,y_test,batch_size=batch_size)
print("Test Accuracy:-> {}".format(acc))
### Evaluation Metrics :


#Method for different classification models
model_perf = model_classification_matrics(model,X_test,y_test)
model_perf
#Confusion matrix method
confusion_matrix_fun(model,X_test,y_test)
### Model Tuning For DNN

#### 1. Early Stopping and Taking the validation data as 30%
#Model with 5 layer with Relu function and dropout after each layer
model_2 = Sequential()
model_2.add(Dense(2048, input_dim=input_size))
model_2.add(Activation('relu'))
model_2.add(Dropout(dropout))
model_2.add(Dense(512))
model_2.add(Activation('relu'))
model_2.add(Dropout(dropout))
model_2.add(Dense(128,input_dim=input_size))
model_2.add(Activation('relu'))
model_2.add(Dropout(dropout))
model_2.add(Dense(32,input_dim=input_size))
model_2.add(Activation('relu'))
model_2.add(Dropout(dropout))
model_2.add(Dense(1))
model_2.add(Activation('sigmoid'))


# Define early stopping criteria
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

#Compile the model again
model_2.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

#Model summary
model_2.summary()
plot_model(model_2,to_file='model_2.png',show_shapes=True)
#Train the model with early Stopping and validation data is 30%
model_2.fit(X_train,y_train,epochs=10,batch_size=batch_size,validation_split=0.3, callbacks=[early_stopping])
#Calculating the accuracy on test data
loss, acc = model_2.evaluate(X_test,y_test,batch_size=batch_size)
print("Test Accuracy:-> {}".format(acc))
#Method for different classification models
model_perf = model_classification_matrics(model_2,X_test,y_test)
model_perf
#Confusion matrix method calling
confusion_matrix_fun(model_2,X_test,y_test)
#### 2. With Weight initialization using HE Initialization
from keras import initializers

#Initializing the He normalization for weight initialization
init_1 = initializers.HeNormal()
init_3 = initializers.HeNormal()
init_4 = initializers.HeNormal()
init_5 = initializers.HeNormal()


#Model with 5 layer with Relu function and dropout after each layer
model_3 = Sequential()
model_3.add(Dense(2048, input_dim=input_size, kernel_initializer=init_1))
model_3.add(Activation('relu'))
model_3.add(Dropout(dropout))
model_3.add(Dense(512, kernel_initializer=init_3))
model_3.add(Activation('relu'))
model_3.add(Dropout(dropout))
model_3.add(Dense(128,input_dim=input_size, kernel_initializer=init_4))
model_3.add(Activation('relu'))
model_3.add(Dropout(dropout))
model_3.add(Dense(32,input_dim=input_size, kernel_initializer=init_5))
model_3.add(Activation('relu'))
model_3.add(Dropout(dropout))
model_3.add(Dense(1))
model_3.add(Activation('sigmoid'))
# Define early stopping criteria
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

#Compile the model again
model_3.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

#Model summary
model_3.summary()
plot_model(model_3,to_file='model_3.png',show_shapes=True)
#Train the model
model_3.fit(X_train,y_train,epochs=10,batch_size=batch_size)
#Calculating the accuracy on test data
loss, acc = model_3.evaluate(X_test,y_test,batch_size=batch_size)
print("Test Accuracy:-> {}".format(acc))
#Method for different classification models
model_perf = model_classification_matrics(model_3,X_test,y_test)
model_perf
#Confusion matrix method calling
confusion_matrix_fun(model_2,X_test,y_test)
### Handling the Class imbalance with Bagging Technique

* created 5 models with with samples
* predicted the values and then mean is taken by averaging
from sklearn.ensemble import BaggingClassifier
from sklearn.utils import resample

# Number of models in the ensemble
num_models = 5
models = []

for i in range(num_models):
    # Create a sample (with replacement) to handle class imbalance
    X_resampled, y_resampled = resample(X_train, y_train, replace=True)

    model = Sequential()
    model.add(Dense(2048, input_dim=input_size))
    model.add(Activation('relu'))
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dense(128,input_dim=input_size))
    model.add(Activation('relu'))
    model.add(Dense(32,input_dim=input_size))
    model.add(Activation('relu'))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))


    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    # Train the model on the  sample
    model.fit(X_resampled, y_resampled, epochs=10, batch_size=32, verbose=0)

    # Add the trained model to the ensemble
    models.append(model)
#Created the np array with zeros
predictions = np.zeros((len(X_test), num_models))

for i, classifier in enumerate(models):
    #Change the array to 1D
    x = classifier.predict(X_test).flatten()
    predictions[:, i] = x
# Aggregate prediction
ensemble_prediction = np.mean(predictions, axis=1)

# Convert prediction to binary class label
ensemble_labels = np.round(ensemble_prediction)

# Evaluate the performance ensemble
accuracy = accuracy_score(y_test, ensemble_labels)
print(f'Ensemble Accuracy:-> {accuracy}')
