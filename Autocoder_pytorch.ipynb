{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtKRtqR6gpU6/u46uQyulz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhimsendabby/Machine--Learning-practice/blob/main/Autocoder_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder with MNIST Dataset\n",
        "\n",
        "This document provides a step-by-step guide to building an autoencoder using the MNIST dataset. The autoencoder is a type of neural network that learns to compress (encode) the data and then reconstruct (decode) it back to the original form.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "2. [Setup & Importing Modules](#setup)\n",
        "3. [Loading Dataset & Data Preprocessing](#data-preprocessing)\n",
        "4. [Building the Autoencoder](#building-the-autoencoder)\n",
        "5. [Training the Autoencoder](#training-the-autoencoder)\n",
        "6. [Evaluating the Autoencoder](#evaluating-the-autoencoder)\n",
        "7. [Conclusion](#conclusion)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). The goal of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, ensure you have the necessary libraries installed. You can install them using pip if you haven't already.\n",
        "\n",
        "```bash\n",
        "pip install tensorflow keras numpy matplotlib\n"
      ],
      "metadata": {
        "id": "WWFshQfyA50M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref:- https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/?ref=lbp"
      ],
      "metadata": {
        "id": "0YviPr0BAzVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Importing Modules"
      ],
      "metadata": {
        "id": "-jwje_Z-8RNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the required libraries\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader,dataset\n",
        "import torchvision\n",
        "from torchvision import transforms,datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-dl7DrD98Unp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "K7s1ZzH49r5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size for iterating images in batching\n",
        "batch_size = 32\n",
        "learning_rate = 1e-1\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "JRFfAFDE9xVF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset & Data Preprocessing"
      ],
      "metadata": {
        "id": "oqutV88692nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading MNIST Dataset and saving in the root directory\n",
        "# Using Transform for converting images into torch tensors\n",
        "train_dataset = datasets.MNIST(root='/content/mnist',train=True,transform=transforms.ToTensor(),download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQkz-L7V8ts7",
        "outputId": "ffa527ea-2d61-4c13-a9ee-c10b30a1d603"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /content/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 17341190.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /content/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /content/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 488550.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /content/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /content/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1388572.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /content/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4028447.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialing DataLoader for loading dataset in batches\n",
        "# 100 images, batch size =10 then dataloader with iterate 100 images by 10 by 10\n",
        "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "7mNUDJwt9CgY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(28*28,128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,36),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(36,18),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(18,9)\n",
        "    )\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(9,18),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(18,36),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(36,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,28*28),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "3Zxxw1tS_Gp0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Autoencoder()\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = learning_rate,\n",
        "                             weight_decay = 1e-8)"
      ],
      "metadata": {
        "id": "oef-0OnJ-b5N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_total_steps = len(train_loader)\n",
        "n_total_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nsf6kl08fBut",
        "outputId": "04eced97-636a-4ad6-d33c-64fd1f89a7b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1875"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = []\n",
        "losses = []\n",
        "for epoch in range(epochs):\n",
        "  for (image, _) in train_loader:\n",
        "    image = image.reshape(-1,28*28)\n",
        "    reconstruct = model(image)\n",
        "\n",
        "    loss = criterion(reconstruct,image)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "  outputs.append((epochs, image, reconstruct))\n",
        "\n",
        "\n",
        "\n",
        "# Defining the Plot Style\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Plotting the last 100 values\n",
        "plt.plot(losses[-100:])"
      ],
      "metadata": {
        "id": "tobNZw88fh7E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}